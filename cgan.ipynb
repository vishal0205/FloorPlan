{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_zip(zip_file_path, extract_to_folder):\n",
    "    # Check if the zip file exists\n",
    "    if not os.path.isfile(zip_file_path):\n",
    "        print(f\"The file {zip_file_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create the destination folder if it does not exist\n",
    "    if not os.path.exists(extract_to_folder):\n",
    "        os.makedirs(extract_to_folder)\n",
    "\n",
    "    try:\n",
    "        # Open the zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Extract all contents to the specified folder\n",
    "            zip_ref.extractall(extract_to_folder)\n",
    "            print(f\"Extracted all files to {extract_to_folder}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"The file {zip_file_path} is not a valid ZIP file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "zip_file_path = r'r (3)\\houseganpp\\vishal\\archive (2).zip'  # Path to your ZIP file\n",
    "extract_to_folder = r'C:\\Users\\Vishal\\OneDrive\\Documents\\design project\\New folder (3)\\houseganpp\\vishal'  # Folder to extract to\n",
    "extract_zip(zip_file_path, extract_to_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "image_size = 256\n",
    "nz = 100  \n",
    "nc = 3    \n",
    "ngf = 64 \n",
    "ndf = 64  \n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "embedding_dim = 100  \n",
    "\n",
    "class FloorplanDataset(Dataset):\n",
    "    def __init__(self, image_dir, tags_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.tags_dir = tags_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(tags_dir, f.replace('png', 'txt')))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.filenames[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        tag_name = os.path.join(self.tags_dir, self.filenames[idx].replace('png', 'txt'))\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            with open(tag_name, 'r', encoding='utf-8') as file:\n",
    "                tags = file.read().replace('\\n', '')\n",
    "        except UnicodeDecodeError:\n",
    "            with open(tag_name, 'r', encoding='cp1252') as file:\n",
    "                tags = file.read().replace('\\n', '')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "       \n",
    "        tag_embedding = torch.zeros(embedding_dim)\n",
    "        words = tags.split()\n",
    "        for word in words:\n",
    "            index = abs(hash(word)) % embedding_dim\n",
    "            tag_embedding[index] = 1\n",
    "\n",
    "        return image, tag_embedding\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# dataset\n",
    "dataset = FloorplanDataset(image_dir=r'C:\\Users\\Vishal\\OneDrive\\Documents\\design project\\New folder (3)\\houseganpp\\vishal\\floorplan_image', tags_dir=r'C:\\Users\\Vishal\\OneDrive\\Documents\\design project\\New folder (3)\\houseganpp\\vishal\\human_annotated_tags', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, embedding_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.text = nn.Linear(embedding_dim, nz)  \n",
    "       \n",
    "        self.main = nn.Sequential(\n",
    "           \n",
    "            nn.ConvTranspose2d(nz * 2, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "          \n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "           \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "           \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, ngf // 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf // 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf // 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text_embedding):\n",
    "       \n",
    "        text_embedding = self.text(text_embedding)\n",
    "        \n",
    "        text_embedding = text_embedding.view(noise.size(0), nz, 1, 1)  \n",
    "\n",
    "       \n",
    "        combined_input = torch.cat([noise, text_embedding], 1)\n",
    "        return self.main(combined_input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, image_size, embedding_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.text = nn.Linear(embedding_dim, image_size * image_size)  \n",
    "        self.main = nn.Sequential(\n",
    "           \n",
    "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            nn.Conv2d(ndf * 32, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, image, text_embedding):\n",
    "       \n",
    "        text_embedding = self.text(text_embedding).view(-1, 1, image_size, image_size)\n",
    "       \n",
    "        input = torch.cat([image, text_embedding], 1)\n",
    "        return self.main(input).view(-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "#Inception model\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "def calculate_fid(real_imgs, fake_imgs, epsilon=1e-6):\n",
    "    inception_model.eval()\n",
    "    with torch.no_grad():\n",
    "        real_activations = inception_model(real_imgs).detach().cpu().numpy()\n",
    "        fake_activations = inception_model(fake_imgs).detach().cpu().numpy()\n",
    "\n",
    "    mu1, sigma1 = real_activations.mean(axis=0), np.cov(real_activations, rowvar=False)\n",
    "    mu2, sigma2 = fake_activations.mean(axis=0), np.cov(fake_activations, rowvar=False)\n",
    "\n",
    "\n",
    "    sigma1 += np.eye(sigma1.shape[0]) * epsilon\n",
    "    sigma2 += np.eye(sigma2.shape[0]) * epsilon\n",
    "\n",
    "    # sum of squared differences between the means\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "\n",
    "    #  sqrt of product of covariance matrices\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Check for complex numbers resulting from numerical inaccuracies and take the real part\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Frechet Inception Distance\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "def calculate_inception_score(imgs, n_split=10):\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(inception_model(imgs), dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # Split predictions and calculate scores\n",
    "    scores = []\n",
    "    n_part = preds.shape[0] // n_split\n",
    "    for i in range(n_split):\n",
    "        part = preds[i * n_part:(i + 1) * n_part, :]\n",
    "        kl_div = part * (np.log(part) - np.log(np.expand_dims(part.mean(axis=0), 0)))\n",
    "        kl_div = kl_div.sum(axis=1)\n",
    "        scores.append(np.exp(kl_div.mean()))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "netG = Generator(nz, ngf, nc, embedding_dim).to(device)\n",
    "netD = Discriminator(ndf, nc, image_size, embedding_dim).to(device)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Noise for input to the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "real_acc_list = []\n",
    "fake_acc_list = []\n",
    "FID_scores = []\n",
    "IS_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    real_correct = 0\n",
    "    fake_correct = 0\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "      \n",
    "        real_labels = torch.full((batch_size,), 0.9, dtype=torch.float, device=device)  # Soft labels for real\n",
    "        fake_labels = torch.full((batch_size,), 0.1, dtype=torch.float, device=device)  # Soft labels for fake\n",
    "\n",
    "        output = netD(real_cpu, data[1].to(device))\n",
    "        errD_real = criterion(output, real_labels)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Calculate real accuracy\n",
    "        pred_real = output.detach() > 0.5  \n",
    "        real_correct += pred_real.sum().item()\n",
    "        total_real += batch_size\n",
    "        \n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise, data[1].to(device))\n",
    "        output = netD(fake.detach(), data[1].to(device))\n",
    "        errD_fake = criterion(output, fake_labels)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # Calculate fake accuracy\n",
    "        pred_fake = output.detach() < 0.5  \n",
    "        fake_correct += pred_fake.sum().item()\n",
    "        total_fake += batch_size\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        D_losses.append(errD.item())\n",
    "        netG.zero_grad()\n",
    "        output = netD(fake, data[1].to(device))\n",
    "        errG = criterion(output, real_labels)  \n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "\n",
    "    # Metric calculation every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            data = next(iter(dataloader))\n",
    "            real_images, embeddings = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Generate fake images for evaluation\n",
    "            fake_images = netG(fixed_noise, embeddings).to(device)\n",
    "            real_images_resized = F.interpolate(real_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            fake_images_resized = F.interpolate(fake_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            fid = calculate_fid(real_images_resized, fake_images_resized)\n",
    "            inception_score = calculate_inception_score(fake_images_resized)\n",
    "\n",
    "            FID_scores.append(fid)\n",
    "            IS_scores.append(inception_score)\n",
    "            print(f'Epoch {epoch+1}: FID = {fid}, IS = {inception_score}')\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    # Calculate accuracies\n",
    "    real_accuracy = real_correct / total_real\n",
    "    fake_accuracy = fake_correct / total_fake\n",
    "    real_acc_list.append(real_accuracy)\n",
    "    fake_acc_list.append(fake_accuracy)\n",
    "    print(f'Epoch {epoch + 1} Real Accuracy: {real_accuracy:.4f}, Fake Accuracy: {fake_accuracy:.4f}')\n",
    "\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        with torch.no_grad():\n",
    "            fake = netG(torch.randn(35, nz, 1, 1, device=device), torch.randn(batch_size, embedding_dim, device=device)).detach().to(device)\n",
    "        output_dir = r'C:\\Users\\Vishal\\OneDrive\\Documents\\design project\\New folder (3)\\houseganpp\\vishal\\output'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_image(fake, rf'C:\\Users\\Vishal\\OneDrive\\Documents\\design project\\New folder (3)\\houseganpp\\vishal\\fakeImages{epoch}.png', normalize=True)\n",
    "\n",
    "torch.save(netG.state_dict(), 'generator.pth')\n",
    "torch.save(netD.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "metric_epochs = list(range(5, num_epochs + 1, 5))\n",
    "iterations = list(range(1, len(D_losses) + 1))\n",
    "\n",
    "# Plotting Discriminator and Generator Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, D_losses, label='Discriminator Loss')\n",
    "plt.plot(iterations, G_losses, label='Generator Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Real and Fake Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, real_acc_list, label='Real Accuracy')\n",
    "plt.plot(epochs, fake_acc_list, label='Fake Accuracy')\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting FID and IS Scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metric_epochs, FID_scores, label='FID')\n",
    "plt.title('Frechet Inception Distance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('FID')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metric_epochs, IS_scores, label='Inception Score')\n",
    "plt.title('Inception Scores')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display  \n",
    "\n",
    "netG = Generator(nz, ngf, nc, embedding_dim).to(device)\n",
    "netG.load_state_dict(torch.load('generator.pth', map_location=device))\n",
    "netG.eval()\n",
    "def generate_image_from_text(text):\n",
    "    tag_embedding = torch.zeros(embedding_dim)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        index = abs(hash(word)) % embedding_dim\n",
    "        tag_embedding[index] = 1\n",
    "    noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "    tag_embedding = tag_embedding.to(device).unsqueeze(0)  \n",
    "\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        fake_image = netG(noise, tag_embedding)\n",
    "    \n",
    "    \n",
    "    fake_image = (fake_image * 0.5 + 0.5) \n",
    "    img = transforms.ToPILImage()(fake_image.squeeze(0).cpu())\n",
    "    img.save(f'generated_image.png') \n",
    "    display(img)\n",
    "\n",
    "generate_image_from_text(\"Balcony Room Located on North east corner with master Room on below , Total size of 11 Feet length and 3 Feet Width. bath Room Located on exact south center of the plan with com Room on right , Total size of 7 Feet length and 3 Feet Width. com Room Located on South east corner with master Room on above , Total size of 13 Feet length and 9 Feet Width. Kitchen Room Located on North west corner with balcony Room on right , Total size of 10 Feet length and 2 Feet Width. Living Room Located on covered entire west area with kitchen Room on above , Total size of 20 Feet length and 18 Feet Width. Master Room Located on North east side with balcony Room on above , Total size of 12 Feet length and 12 Feet Width. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myeven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
